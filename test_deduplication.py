"""
å›¾ç‰‡å»é‡å¤åŠŸèƒ½æµ‹è¯•è„šæœ¬

è¿™ä¸ªè„šæœ¬æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨å»é‡å¤åŠŸèƒ½ï¼š
1. æ£€æµ‹é‡å¤å›¾ç‰‡
2. ç”Ÿæˆè¿‡æ»¤åˆ—è¡¨
3. é‡å»ºç´¢å¼•æ—¶åº”ç”¨è¿‡æ»¤
"""

import os
import json
from src.deduplication import (
    DuplicateDetector, DuplicateSelector, FilterListGenerator, DeduplicationReport, 
    FILTER_LIST_PATH, DEDUP_REPORT_PATH
)
from src.search import SearchEngine
from src.config import DATA_DIR


def print_section(title):
    """æ‰“å°ç« èŠ‚æ ‡é¢˜"""
    print("\n" + "=" * 80)
    print(f"  {title}")
    print("=" * 80)


def print_subsection(title):
    """æ‰“å°å°èŠ‚æ ‡é¢˜"""
    print(f"\n>>> {title}")


def test_deduplication_workflow():
    """å®Œæ•´çš„å»é‡å¤å·¥ä½œæµæµ‹è¯•"""
    
    print_section("å›¾ç‰‡å»é‡å¤åŠŸèƒ½æµ‹è¯•")
    
    # æ­¥éª¤1ï¼šæ£€æµ‹é‡å¤å›¾ç‰‡
    print_subsection("æ­¥éª¤1: æ£€æµ‹é‡å¤å›¾ç‰‡ç»„")
    
    detector = DuplicateDetector(similarity_threshold=0.95)
    
    if not detector.load_features_from_db():
        print("âŒ é”™è¯¯ï¼šæ— æ³•åŠ è½½ç‰¹å¾å‘é‡ã€‚è¯·å…ˆè¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š")
        print("   python main.py init")
        print("   python main.py scan <image_directory>")
        print("   python main.py process")
        return
    
    if not detector.build_faiss_index():
        print("âŒ é”™è¯¯ï¼šæ— æ³•æ„å»ºç´¢å¼•")
        return
    
    duplicate_groups_dict = detector.find_duplicate_groups()
    merged_groups = detector.merge_duplicate_groups(duplicate_groups_dict)
    
    print(f"\nâœ“ æ£€æµ‹å®Œæˆ")
    print(f"  æ€»å›¾ç‰‡æ•°: {len(detector.image_paths)}")
    print(f"  é‡å¤ç»„æ•°: {len(merged_groups)}")
    
    if not merged_groups:
        print("\n â„¹ï¸  æœªæ£€æµ‹åˆ°é‡å¤å›¾ç‰‡")
        return
    
    # æ­¥éª¤2ï¼šç­›é€‰æœ€ä½³å›¾ç‰‡
    print_subsection("æ­¥éª¤2: ç­›é€‰å„ç»„å†…æœ€ä½³å›¾ç‰‡ä¿ç•™")
    
    selector = DuplicateSelector(detector.image_paths)
    retained_paths, filtered_paths = selector.select_best_from_groups(merged_groups)
    
    print(f"\nâœ“ ç­›é€‰å®Œæˆ")
    print(f"  ä¿ç•™å›¾ç‰‡: {len(retained_paths)} å¼ ")
    print(f"  å¾…è¿‡æ»¤å›¾ç‰‡: {len(filtered_paths)} å¼ ")
    
    # æ­¥éª¤3ï¼šç”Ÿæˆè¿‡æ»¤åˆ—è¡¨
    print_subsection("æ­¥éª¤3: ç”Ÿæˆè¿‡æ»¤åˆ—è¡¨å’ŒæŠ¥å‘Š")
    
    if FilterListGenerator.generate_filter_list(filtered_paths):
        print(f"âœ“ è¿‡æ»¤åˆ—è¡¨å·²ç”Ÿæˆ: {FILTER_LIST_PATH}")
    else:
        print("âŒ ç”Ÿæˆè¿‡æ»¤åˆ—è¡¨å¤±è´¥")
        return
    
    if DeduplicationReport.generate_report(merged_groups, detector.image_paths, 
                                          filtered_paths, retained_paths):
        print(f"âœ“ å»é‡æŠ¥å‘Šå·²ç”Ÿæˆ: {DEDUP_REPORT_PATH}")
    else:
        print("âŒ ç”Ÿæˆå»é‡æŠ¥å‘Šå¤±è´¥")
        return
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    print_subsection("ç»Ÿè®¡ä¿¡æ¯")
    print(f"\næ€»ç»“:")
    print(f"  â€¢ æ€»å›¾ç‰‡æ•°: {len(detector.image_paths)}")
    print(f"  â€¢ é‡å¤ç»„æ•°: {len(merged_groups)}")
    print(f"  â€¢ ä¿ç•™å›¾ç‰‡: {len(retained_paths)} å¼  ({len(retained_paths)/len(detector.image_paths)*100:.1f}%)")
    print(f"  â€¢ å¾…è¿‡æ»¤å›¾ç‰‡: {len(filtered_paths)} å¼  ({len(filtered_paths)/len(detector.image_paths)*100:.1f}%)")
    print(f"  â€¢ å»é‡ç‡: {len(filtered_paths)/len(detector.image_paths)*100:.1f}%")
    
    # æ˜¾ç¤ºä¸€äº›é‡å¤ç»„çš„è¯¦æƒ…
    print_subsection("é‡å¤ç»„ç¤ºä¾‹")
    for i, group in enumerate(merged_groups[:3]):  # æ˜¾ç¤ºå‰3ä¸ªç»„
        group_list = sorted(list(group))
        print(f"\n  ç»„ {i+1} ({len(group)} å¼ ):")
        for img_id in group_list:
            size = os.path.getsize(detector.image_paths[img_id]) / 1024
            marker = " â† ä¿ç•™" if detector.image_paths[img_id] in retained_paths else " â† å¾…è¿‡æ»¤"
            print(f"    â€¢ {detector.image_paths[img_id]} ({size:.1f}KB){marker}")
    
    if len(merged_groups) > 3:
        print(f"\n  ... è¿˜æœ‰ {len(merged_groups) - 3} ä¸ªé‡å¤ç»„")


def test_rebuild_index_with_filter():
    """æµ‹è¯•ä½¿ç”¨è¿‡æ»¤åˆ—è¡¨é‡å»ºç´¢å¼•"""
    
    print_section("æµ‹è¯•é‡å»ºç´¢å¼•ï¼ˆåº”ç”¨è¿‡æ»¤åˆ—è¡¨ï¼‰")
    
    print_subsection("æ£€æŸ¥è¿‡æ»¤åˆ—è¡¨")
    
    if not os.path.exists(FILTER_LIST_PATH):
        print("âŒ è¿‡æ»¤åˆ—è¡¨ä¸å­˜åœ¨ã€‚è¯·å…ˆè¿è¡Œå»é‡å¤æµç¨‹ã€‚")
        return
    
    with open(FILTER_LIST_PATH, 'r', encoding='utf-8') as f:
        filter_data = json.load(f)
    
    print(f"âœ“ è¿‡æ»¤åˆ—è¡¨å·²åŠ è½½")
    print(f"  å¾…è¿‡æ»¤å›¾ç‰‡æ•°: {filter_data['total_filtered']}")
    
    print_subsection("é‡å»ºç´¢å¼•ï¼ˆåº”ç”¨è¿‡æ»¤ï¼‰")
    
    engine = SearchEngine()
    if engine.build_index(apply_filter=True):
        print(f"âœ“ ç´¢å¼•é‡å»ºæˆåŠŸ")
        print(f"  ç´¢å¼•ä¸­çš„å›¾ç‰‡æ•°: {engine.index.ntotal}")
        print(f"  è¿‡æ»¤æ‰çš„é‡å¤å›¾ç‰‡: {filter_data['total_filtered']}")
        
        # ä¿å­˜ç´¢å¼•
        if engine.save_index():
            print(f"âœ“ ç´¢å¼•å·²ä¿å­˜")
        else:
            print("âŒ ç´¢å¼•ä¿å­˜å¤±è´¥")
    else:
        print("âŒ ç´¢å¼•é‡å»ºå¤±è´¥")
        return
    
    print_subsection("éªŒè¯ç»“æœ")
    print(f"\nâœ“ æ–°ç´¢å¼•åªåŒ…å«éé‡å¤çš„å›¾ç‰‡")
    print(f"  åŸå§‹å›¾ç‰‡æ€»æ•°: {filter_data['total_filtered'] + engine.index.ntotal}")
    print(f"  ç´¢å¼•ä¸­çš„å›¾ç‰‡æ•°: {engine.index.ntotal}")
    print(f"  é‡å¤è¢«æ’é™¤çš„å›¾ç‰‡: {filter_data['total_filtered']}")


def view_dedup_report():
    """æŸ¥çœ‹å»é‡æŠ¥å‘Š"""
    
    if not os.path.exists(DEDUP_REPORT_PATH):
        print("âŒ å»é‡æŠ¥å‘Šä¸å­˜åœ¨ã€‚è¯·å…ˆè¿è¡Œå»é‡å¤æµç¨‹ã€‚")
        return
    
    with open(DEDUP_REPORT_PATH, 'r', encoding='utf-8') as f:
        report = json.load(f)
    
    print_section("å»é‡å¤æŠ¥å‘Šè¯¦æƒ…")
    
    summary = report['summary']
    print(f"\næ€»ç»“:")
    print(f"  â€¢ æ€»å›¾ç‰‡æ•°: {summary['total_images']}")
    print(f"  â€¢ é‡å¤ç»„æ•°: {summary['duplicate_groups']}")
    print(f"  â€¢ å¾…è¿‡æ»¤å›¾ç‰‡: {summary['filtered_count']}")
    print(f"  â€¢ ä¿ç•™å›¾ç‰‡: {summary['retained_count']}")
    
    if summary['duplicate_groups'] > 0:
        print(f"\nå‰ 5 ä¸ªé‡å¤ç»„:")
        for group in report['duplicate_groups'][:5]:
            print(f"\n  ç»„ {group['group_id']} ({group['size']} å¼ ):")
            for img_path in group['images'][:3]:
                print(f"    â€¢ {os.path.basename(img_path)}")
            if group['size'] > 3:
                print(f"    ... è¿˜æœ‰ {group['size'] - 3} å¼ ")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        if sys.argv[1] == "report":
            view_dedup_report()
        elif sys.argv[1] == "rebuild":
            test_rebuild_index_with_filter()
        else:
            print("æœªçŸ¥å‚æ•°ã€‚ç”¨æ³•:")
            print("  python test_deduplication.py          # å®Œæ•´å·¥ä½œæµæµ‹è¯•")
            print("  python test_deduplication.py report   # æŸ¥çœ‹å»é‡æŠ¥å‘Š")
            print("  python test_deduplication.py rebuild  # æµ‹è¯•ç´¢å¼•é‡å»º")
    else:
        test_deduplication_workflow()
        print("\n" + "=" * 80)
        print("ğŸ’¡ ä¸‹ä¸€æ­¥å»ºè®®:")
        print("=" * 80)
        print("  1. æŸ¥çœ‹å»é‡æŠ¥å‘Š: python test_deduplication.py report")
        print("  2. é‡å»ºç´¢å¼•åº”ç”¨è¿‡æ»¤: python test_deduplication.py rebuild")
        print('  3. æŸ¥çœ‹è¿‡æ»¤åˆ—è¡¨: cat data/filter_list.json')
